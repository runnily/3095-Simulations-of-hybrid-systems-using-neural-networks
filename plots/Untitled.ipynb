{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unlike-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(1, \"../\")\n",
    "from simulations import lorenz_system\n",
    "from prototype_nn import Splitting, prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ongoing-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lorenz_system(0.01, 100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bottom-notification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>initial_x</th>\n",
       "      <th>initial_y</th>\n",
       "      <th>initial_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1.012566</td>\n",
       "      <td>1.259920</td>\n",
       "      <td>0.984891</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02</td>\n",
       "      <td>1.048821</td>\n",
       "      <td>1.524001</td>\n",
       "      <td>0.973114</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03</td>\n",
       "      <td>1.107206</td>\n",
       "      <td>1.798315</td>\n",
       "      <td>0.965159</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.04</td>\n",
       "      <td>1.186865</td>\n",
       "      <td>2.088546</td>\n",
       "      <td>0.961737</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>99.95</td>\n",
       "      <td>-2.914424</td>\n",
       "      <td>3.248325</td>\n",
       "      <td>29.495048</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>99.96</td>\n",
       "      <td>-2.327941</td>\n",
       "      <td>3.244032</td>\n",
       "      <td>28.635080</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>99.97</td>\n",
       "      <td>-1.798881</td>\n",
       "      <td>3.216650</td>\n",
       "      <td>27.815939</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>99.98</td>\n",
       "      <td>-1.323485</td>\n",
       "      <td>3.175996</td>\n",
       "      <td>27.034886</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>99.99</td>\n",
       "      <td>-0.897517</td>\n",
       "      <td>3.129892</td>\n",
       "      <td>26.289067</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>640000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       time         x         y          z  initial_x  initial_y  initial_z\n",
       "0      0.00  1.000000  1.000000   1.000000          1          1          1\n",
       "1      0.01  1.012566  1.259920   0.984891          1          1          1\n",
       "2      0.02  1.048821  1.524001   0.973114          1          1          1\n",
       "3      0.03  1.107206  1.798315   0.965159          1          1          1\n",
       "4      0.04  1.186865  2.088546   0.961737          1          1          1\n",
       "...     ...       ...       ...        ...        ...        ...        ...\n",
       "9995  99.95 -2.914424  3.248325  29.495048          4          4          4\n",
       "9996  99.96 -2.327941  3.244032  28.635080          4          4          4\n",
       "9997  99.97 -1.798881  3.216650  27.815939          4          4          4\n",
       "9998  99.98 -1.323485  3.175996  27.034886          4          4          4\n",
       "9999  99.99 -0.897517  3.129892  26.289067          4          4          4\n",
       "\n",
       "[640000 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "designed-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = Splitting([0,4,5,6], [1,2,3], num_epoches = 600, batch_size = 512, lr= 0.00000005) #0.0000012 #0.00000012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "upset-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = kfold.split(df, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "religious-round",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>initial_x</th>\n",
       "      <th>initial_y</th>\n",
       "      <th>initial_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1.13</td>\n",
       "      <td>-7.199852</td>\n",
       "      <td>-6.861171</td>\n",
       "      <td>25.934951</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>13.38</td>\n",
       "      <td>1.230035</td>\n",
       "      <td>2.686719</td>\n",
       "      <td>20.009090</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7204</th>\n",
       "      <td>72.04</td>\n",
       "      <td>14.774597</td>\n",
       "      <td>9.449465</td>\n",
       "      <td>40.147163</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5147</th>\n",
       "      <td>51.47</td>\n",
       "      <td>0.358709</td>\n",
       "      <td>0.510554</td>\n",
       "      <td>13.290834</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2528</th>\n",
       "      <td>25.28</td>\n",
       "      <td>-2.111215</td>\n",
       "      <td>-4.226151</td>\n",
       "      <td>5.602879</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9561</th>\n",
       "      <td>95.61</td>\n",
       "      <td>-1.077513</td>\n",
       "      <td>-1.929653</td>\n",
       "      <td>15.668729</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9573</th>\n",
       "      <td>95.73</td>\n",
       "      <td>11.484643</td>\n",
       "      <td>16.922102</td>\n",
       "      <td>23.412809</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1854</th>\n",
       "      <td>18.54</td>\n",
       "      <td>-2.381287</td>\n",
       "      <td>3.335270</td>\n",
       "      <td>28.810905</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>16.99</td>\n",
       "      <td>0.440985</td>\n",
       "      <td>-0.634005</td>\n",
       "      <td>20.439366</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>14.02</td>\n",
       "      <td>12.998273</td>\n",
       "      <td>9.182808</td>\n",
       "      <td>36.566103</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>512000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       time          x          y          z  initial_x  initial_y  initial_z\n",
       "113    1.13  -7.199852  -6.861171  25.934951          1          4          1\n",
       "1338  13.38   1.230035   2.686719  20.009090          3          3          2\n",
       "7204  72.04  14.774597   9.449465  40.147163          3          3          3\n",
       "5147  51.47   0.358709   0.510554  13.290834          3          3          1\n",
       "2528  25.28  -2.111215  -4.226151   5.602879          4          2          1\n",
       "...     ...        ...        ...        ...        ...        ...        ...\n",
       "9561  95.61  -1.077513  -1.929653  15.668729          3          1          2\n",
       "9573  95.73  11.484643  16.922102  23.412809          2          4          2\n",
       "1854  18.54  -2.381287   3.335270  28.810905          1          3          1\n",
       "1699  16.99   0.440985  -0.634005  20.439366          4          1          2\n",
       "1402  14.02  12.998273   9.182808  36.566103          3          2          1\n",
       "\n",
       "[512000 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "binary-group",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>initial_x</th>\n",
       "      <th>initial_y</th>\n",
       "      <th>initial_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4628</th>\n",
       "      <td>46.28</td>\n",
       "      <td>-2.867591</td>\n",
       "      <td>-2.821095</td>\n",
       "      <td>20.493879</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>24.83</td>\n",
       "      <td>-10.753976</td>\n",
       "      <td>-10.100381</td>\n",
       "      <td>30.619953</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6356</th>\n",
       "      <td>63.56</td>\n",
       "      <td>-7.182149</td>\n",
       "      <td>-2.031770</td>\n",
       "      <td>31.389924</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4096</th>\n",
       "      <td>40.96</td>\n",
       "      <td>7.951758</td>\n",
       "      <td>11.235216</td>\n",
       "      <td>21.427621</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>19.90</td>\n",
       "      <td>-10.946534</td>\n",
       "      <td>-16.737423</td>\n",
       "      <td>21.742526</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>18.31</td>\n",
       "      <td>-6.286232</td>\n",
       "      <td>-9.358394</td>\n",
       "      <td>18.371750</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5692</th>\n",
       "      <td>56.92</td>\n",
       "      <td>1.677939</td>\n",
       "      <td>0.214066</td>\n",
       "      <td>22.437371</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364</th>\n",
       "      <td>33.64</td>\n",
       "      <td>-12.562280</td>\n",
       "      <td>-20.865397</td>\n",
       "      <td>20.540269</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2182</th>\n",
       "      <td>21.82</td>\n",
       "      <td>0.984164</td>\n",
       "      <td>0.092843</td>\n",
       "      <td>20.479234</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>6.67</td>\n",
       "      <td>-6.656929</td>\n",
       "      <td>-4.831634</td>\n",
       "      <td>27.310445</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       time          x          y          z  initial_x  initial_y  initial_z\n",
       "4628  46.28  -2.867591  -2.821095  20.493879          3          2          2\n",
       "2483  24.83 -10.753976 -10.100381  30.619953          3          4          2\n",
       "6356  63.56  -7.182149  -2.031770  31.389924          1          4          3\n",
       "4096  40.96   7.951758  11.235216  21.427621          4          3          4\n",
       "1990  19.90 -10.946534 -16.737423  21.742526          2          1          1\n",
       "...     ...        ...        ...        ...        ...        ...        ...\n",
       "1831  18.31  -6.286232  -9.358394  18.371750          1          1          4\n",
       "5692  56.92   1.677939   0.214066  22.437371          3          4          1\n",
       "3364  33.64 -12.562280 -20.865397  20.540269          4          4          3\n",
       "2182  21.82   0.984164   0.092843  20.479234          4          3          1\n",
       "667    6.67  -6.656929  -4.831634  27.310445          4          2          1\n",
       "\n",
       "[128000 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-aerospace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Fold 0 -------------\n",
      "TEST: 0 TRAIN: [1, 2, 3, 4]\n",
      "Epoch: 0  Train Loss: 266.513916015625 \n",
      "Epoch: 1  Train Loss: 242.2711639404297 \n",
      "Epoch: 2  Train Loss: 253.2183074951172 \n",
      "Epoch: 3  Train Loss: 253.9022216796875 \n",
      "Epoch: 4  Train Loss: 253.73387145996094 \n",
      "Epoch: 5  Train Loss: 239.72010803222656 \n",
      "Epoch: 6  Train Loss: 243.09629821777344 \n",
      "Epoch: 7  Train Loss: 237.2686004638672 \n",
      "Epoch: 8  Train Loss: 245.24375915527344 \n",
      "Epoch: 9  Train Loss: 241.7353515625 \n",
      "Epoch: 10  Train Loss: 253.41319274902344 \n",
      "Epoch: 11  Train Loss: 250.47911071777344 \n",
      "Epoch: 12  Train Loss: 247.5594024658203 \n",
      "Epoch: 13  Train Loss: 240.6302032470703 \n",
      "Epoch: 14  Train Loss: 230.43896484375 \n",
      "Epoch: 15  Train Loss: 228.0309295654297 \n",
      "Epoch: 16  Train Loss: 233.1243438720703 \n",
      "Epoch: 17  Train Loss: 239.3675537109375 \n",
      "Epoch: 18  Train Loss: 233.06103515625 \n",
      "Epoch: 19  Train Loss: 221.9387969970703 \n",
      "Epoch: 20  Train Loss: 228.7574005126953 \n",
      "Epoch: 21  Train Loss: 239.24952697753906 \n",
      "Epoch: 22  Train Loss: 240.8645477294922 \n",
      "Epoch: 23  Train Loss: 234.06396484375 \n",
      "Epoch: 24  Train Loss: 230.2014923095703 \n",
      "Epoch: 25  Train Loss: 238.4080047607422 \n",
      "Epoch: 26  Train Loss: 235.2030792236328 \n",
      "Epoch: 27  Train Loss: 229.7573699951172 \n",
      "Epoch: 28  Train Loss: 217.7992706298828 \n",
      "Epoch: 29  Train Loss: 216.4838104248047 \n",
      "Epoch: 30  Train Loss: 221.3939971923828 \n",
      "Epoch: 31  Train Loss: 226.9994659423828 \n",
      "Epoch: 32  Train Loss: 232.4478302001953 \n",
      "Epoch: 33  Train Loss: 240.23069763183594 \n",
      "Epoch: 34  Train Loss: 220.3286590576172 \n",
      "Epoch: 35  Train Loss: 219.69061279296875 \n",
      "Epoch: 36  Train Loss: 217.62904357910156 \n",
      "Epoch: 37  Train Loss: 221.68572998046875 \n",
      "Epoch: 38  Train Loss: 224.50987243652344 \n",
      "Epoch: 39  Train Loss: 209.50108337402344 \n",
      "Epoch: 40  Train Loss: 213.8944549560547 \n",
      "Epoch: 41  Train Loss: 213.8711700439453 \n",
      "Epoch: 42  Train Loss: 216.6995086669922 \n",
      "Epoch: 43  Train Loss: 218.88262939453125 \n",
      "Epoch: 44  Train Loss: 209.9768524169922 \n",
      "Epoch: 45  Train Loss: 206.67591857910156 \n",
      "Epoch: 46  Train Loss: 202.2254638671875 \n",
      "Epoch: 47  Train Loss: 211.4036407470703 \n",
      "Epoch: 48  Train Loss: 221.6894073486328 \n",
      "Epoch: 49  Train Loss: 209.77447509765625 \n",
      "Epoch: 50  Train Loss: 198.78834533691406 \n",
      "Epoch: 51  Train Loss: 209.3868865966797 \n",
      "Epoch: 52  Train Loss: 207.178466796875 \n",
      "Epoch: 53  Train Loss: 207.06602478027344 \n",
      "Epoch: 54  Train Loss: 204.49085998535156 \n",
      "Epoch: 55  Train Loss: 191.47650146484375 \n",
      "Epoch: 56  Train Loss: 194.4872589111328 \n",
      "Epoch: 57  Train Loss: 214.2217559814453 \n",
      "Epoch: 58  Train Loss: 200.7089080810547 \n",
      "Epoch: 59  Train Loss: 196.7382049560547 \n",
      "Epoch: 60  Train Loss: 202.08534240722656 \n",
      "Epoch: 61  Train Loss: 179.685546875 \n",
      "Epoch: 62  Train Loss: 180.92274475097656 \n",
      "Epoch: 63  Train Loss: 191.04612731933594 \n",
      "Epoch: 64  Train Loss: 189.6334686279297 \n",
      "Epoch: 65  Train Loss: 195.74684143066406 \n",
      "Epoch: 66  Train Loss: 191.858154296875 \n",
      "Epoch: 67  Train Loss: 186.3170623779297 \n",
      "Epoch: 68  Train Loss: 193.99713134765625 \n",
      "Epoch: 69  Train Loss: 176.24156188964844 \n",
      "Epoch: 70  Train Loss: 188.3406219482422 \n",
      "Epoch: 71  Train Loss: 179.7633514404297 \n",
      "Epoch: 72  Train Loss: 173.2369842529297 \n",
      "Epoch: 73  Train Loss: 182.94203186035156 \n",
      "Epoch: 74  Train Loss: 169.24790954589844 \n",
      "Epoch: 75  Train Loss: 190.1125030517578 \n",
      "Epoch: 76  Train Loss: 180.24427795410156 \n",
      "Epoch: 77  Train Loss: 191.9485321044922 \n",
      "Epoch: 78  Train Loss: 177.61891174316406 \n",
      "Epoch: 79  Train Loss: 176.56739807128906 \n",
      "Epoch: 80  Train Loss: 178.05255126953125 \n",
      "Epoch: 81  Train Loss: 181.8952178955078 \n",
      "Epoch: 82  Train Loss: 166.59423828125 \n",
      "Epoch: 83  Train Loss: 164.4801025390625 \n",
      "Epoch: 84  Train Loss: 159.85118103027344 \n",
      "Epoch: 85  Train Loss: 172.34400939941406 \n",
      "Epoch: 86  Train Loss: 175.9802703857422 \n",
      "Epoch: 87  Train Loss: 164.61163330078125 \n",
      "Epoch: 88  Train Loss: 181.33534240722656 \n",
      "Epoch: 89  Train Loss: 167.3531036376953 \n",
      "Epoch: 90  Train Loss: 163.6414031982422 \n",
      "Epoch: 91  Train Loss: 162.81088256835938 \n",
      "Epoch: 92  Train Loss: 171.5 \n",
      "Epoch: 93  Train Loss: 160.04110717773438 \n",
      "Epoch: 94  Train Loss: 152.78211975097656 \n",
      "Epoch: 95  Train Loss: 164.43882751464844 \n",
      "Epoch: 96  Train Loss: 165.7030487060547 \n",
      "Epoch: 97  Train Loss: 169.6358184814453 \n",
      "Epoch: 98  Train Loss: 151.9334259033203 \n",
      "Epoch: 99  Train Loss: 160.24217224121094 \n",
      "Epoch: 100  Train Loss: 153.2080535888672 \n",
      "Epoch: 101  Train Loss: 169.6923065185547 \n",
      "Epoch: 102  Train Loss: 154.97267150878906 \n",
      "Epoch: 103  Train Loss: 152.81446838378906 \n",
      "Epoch: 104  Train Loss: 157.89703369140625 \n",
      "Epoch: 105  Train Loss: 149.992431640625 \n",
      "Epoch: 106  Train Loss: 153.8673553466797 \n",
      "Epoch: 107  Train Loss: 157.964599609375 \n",
      "Epoch: 108  Train Loss: 140.99398803710938 \n",
      "Epoch: 109  Train Loss: 152.19927978515625 \n",
      "Epoch: 110  Train Loss: 153.90101623535156 \n",
      "Epoch: 111  Train Loss: 136.780517578125 \n",
      "Epoch: 112  Train Loss: 147.5437774658203 \n",
      "Epoch: 113  Train Loss: 147.75 \n",
      "Epoch: 114  Train Loss: 140.93174743652344 \n",
      "Epoch: 115  Train Loss: 139.0794677734375 \n",
      "Epoch: 116  Train Loss: 142.1121063232422 \n",
      "Epoch: 117  Train Loss: 137.5059356689453 \n",
      "Epoch: 118  Train Loss: 139.7880401611328 \n",
      "Epoch: 119  Train Loss: 130.61402893066406 \n",
      "Epoch: 120  Train Loss: 125.56683349609375 \n",
      "Epoch: 121  Train Loss: 146.1314697265625 \n",
      "Epoch: 122  Train Loss: 138.37619018554688 \n",
      "Epoch: 123  Train Loss: 147.76422119140625 \n",
      "Epoch: 124  Train Loss: 137.9373016357422 \n",
      "Epoch: 125  Train Loss: 137.9556121826172 \n",
      "Epoch: 126  Train Loss: 135.16873168945312 \n",
      "Epoch: 127  Train Loss: 133.1850128173828 \n",
      "Epoch: 128  Train Loss: 130.6109161376953 \n",
      "Epoch: 129  Train Loss: 125.84445190429688 \n",
      "Epoch: 130  Train Loss: 126.59937286376953 \n",
      "Epoch: 131  Train Loss: 131.73899841308594 \n",
      "Epoch: 132  Train Loss: 141.96527099609375 \n",
      "Epoch: 133  Train Loss: 124.55622100830078 \n",
      "Epoch: 134  Train Loss: 117.8235855102539 \n",
      "Epoch: 135  Train Loss: 128.9885711669922 \n",
      "Epoch: 136  Train Loss: 124.43306732177734 \n",
      "Epoch: 137  Train Loss: 128.4432830810547 \n",
      "Epoch: 138  Train Loss: 125.15143585205078 \n",
      "Epoch: 139  Train Loss: 124.69576263427734 \n",
      "Epoch: 140  Train Loss: 125.03180694580078 \n",
      "Epoch: 141  Train Loss: 123.67144775390625 \n",
      "Epoch: 142  Train Loss: 121.622802734375 \n",
      "Epoch: 143  Train Loss: 118.0503921508789 \n",
      "Epoch: 144  Train Loss: 126.2474594116211 \n",
      "Epoch: 145  Train Loss: 121.56597900390625 \n",
      "Epoch: 146  Train Loss: 128.13389587402344 \n",
      "Epoch: 147  Train Loss: 120.51729583740234 \n",
      "Epoch: 148  Train Loss: 129.29251098632812 \n",
      "Epoch: 149  Train Loss: 126.78375244140625 \n",
      "Epoch: 150  Train Loss: 115.50418090820312 \n",
      "Epoch: 151  Train Loss: 123.78404998779297 \n",
      "Epoch: 152  Train Loss: 122.95304107666016 \n",
      "Epoch: 153  Train Loss: 115.77426147460938 \n",
      "Epoch: 154  Train Loss: 124.8246078491211 \n",
      "Epoch: 155  Train Loss: 122.39273834228516 \n",
      "Epoch: 156  Train Loss: 125.50222778320312 \n",
      "Epoch: 157  Train Loss: 116.5031967163086 \n",
      "Epoch: 158  Train Loss: 112.94416046142578 \n",
      "Epoch: 159  Train Loss: 121.46380615234375 \n",
      "Epoch: 160  Train Loss: 115.62802124023438 \n",
      "Epoch: 161  Train Loss: 124.48226928710938 \n",
      "Epoch: 162  Train Loss: 120.4321060180664 \n",
      "Epoch: 163  Train Loss: 121.6269760131836 \n",
      "Epoch: 164  Train Loss: 128.43618774414062 \n",
      "Epoch: 165  Train Loss: 119.1991958618164 \n",
      "Epoch: 166  Train Loss: 111.2263412475586 \n",
      "Epoch: 167  Train Loss: 121.4577865600586 \n",
      "Epoch: 168  Train Loss: 119.45590209960938 \n",
      "Epoch: 169  Train Loss: 119.14519500732422 \n",
      "Epoch: 170  Train Loss: 119.39665985107422 \n",
      "Epoch: 171  Train Loss: 116.36185455322266 \n",
      "Epoch: 172  Train Loss: 120.22466278076172 \n",
      "Epoch: 173  Train Loss: 121.64868927001953 \n",
      "Epoch: 174  Train Loss: 117.54862213134766 \n",
      "Epoch: 175  Train Loss: 114.99156951904297 \n",
      "Epoch: 176  Train Loss: 113.89946746826172 \n",
      "Epoch: 177  Train Loss: 125.67304229736328 \n",
      "Epoch: 178  Train Loss: 112.3382568359375 \n",
      "Epoch: 179  Train Loss: 119.33984375 \n",
      "Epoch: 180  Train Loss: 117.98827362060547 \n",
      "Epoch: 181  Train Loss: 117.6375732421875 \n",
      "Epoch: 182  Train Loss: 116.33926391601562 \n",
      "Epoch: 183  Train Loss: 117.61199951171875 \n",
      "Epoch: 184  Train Loss: 118.49639129638672 \n",
      "Epoch: 185  Train Loss: 120.4749755859375 \n",
      "Epoch: 186  Train Loss: 123.74615478515625 \n",
      "Epoch: 187  Train Loss: 113.56707763671875 \n",
      "Epoch: 188  Train Loss: 105.19509887695312 \n",
      "Epoch: 189  Train Loss: 124.3271484375 \n",
      "Epoch: 190  Train Loss: 131.37417602539062 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 191  Train Loss: 122.03595733642578 \n",
      "Epoch: 192  Train Loss: 115.917236328125 \n",
      "Epoch: 193  Train Loss: 124.69308471679688 \n",
      "Epoch: 194  Train Loss: 119.48736572265625 \n",
      "Epoch: 195  Train Loss: 114.3324203491211 \n",
      "Epoch: 196  Train Loss: 125.11968994140625 \n",
      "Epoch: 197  Train Loss: 117.0763931274414 \n",
      "Epoch: 198  Train Loss: 120.1742172241211 \n",
      "Epoch: 199  Train Loss: 122.83541107177734 \n",
      "Epoch: 200  Train Loss: 120.4626693725586 \n",
      "Epoch: 201  Train Loss: 117.35521697998047 \n",
      "Epoch: 202  Train Loss: 116.52679443359375 \n",
      "Epoch: 203  Train Loss: 119.91522216796875 \n",
      "Epoch: 204  Train Loss: 126.73540496826172 \n",
      "Epoch: 205  Train Loss: 120.76446533203125 \n",
      "Epoch: 206  Train Loss: 122.43566131591797 \n",
      "Epoch: 207  Train Loss: 115.26007080078125 \n",
      "Epoch: 208  Train Loss: 124.50821685791016 \n",
      "Epoch: 209  Train Loss: 112.66934204101562 \n",
      "Epoch: 210  Train Loss: 105.8019027709961 \n",
      "Epoch: 211  Train Loss: 119.75457000732422 \n",
      "Epoch: 212  Train Loss: 117.5562515258789 \n",
      "Epoch: 213  Train Loss: 113.7256088256836 \n",
      "Epoch: 214  Train Loss: 117.3207778930664 \n",
      "Epoch: 215  Train Loss: 114.8302993774414 \n",
      "Epoch: 216  Train Loss: 125.67288970947266 \n",
      "Epoch: 217  Train Loss: 119.5417251586914 \n",
      "Epoch: 218  Train Loss: 114.68875122070312 \n",
      "Epoch: 219  Train Loss: 115.27740478515625 \n",
      "Epoch: 220  Train Loss: 126.11588287353516 \n",
      "Epoch: 221  Train Loss: 117.33853912353516 \n",
      "Epoch: 222  Train Loss: 115.77274322509766 \n",
      "Epoch: 223  Train Loss: 114.50763702392578 \n",
      "Epoch: 224  Train Loss: 117.5854263305664 \n",
      "Epoch: 225  Train Loss: 115.8232192993164 \n",
      "Epoch: 226  Train Loss: 115.27873992919922 \n",
      "Epoch: 227  Train Loss: 120.87841033935547 \n",
      "Epoch: 228  Train Loss: 116.02838134765625 \n",
      "Epoch: 229  Train Loss: 116.20162963867188 \n",
      "Epoch: 230  Train Loss: 114.11808013916016 \n",
      "Epoch: 231  Train Loss: 124.64373779296875 \n",
      "Epoch: 232  Train Loss: 119.9984359741211 \n",
      "Epoch: 233  Train Loss: 117.07491302490234 \n",
      "Epoch: 234  Train Loss: 118.8851318359375 \n",
      "Epoch: 235  Train Loss: 119.97904205322266 \n",
      "Epoch: 236  Train Loss: 112.73480224609375 \n",
      "Epoch: 237  Train Loss: 113.56988525390625 \n",
      "Epoch: 238  Train Loss: 110.75264739990234 \n",
      "Epoch: 239  Train Loss: 112.5058822631836 \n",
      "Epoch: 240  Train Loss: 119.4891586303711 \n",
      "Epoch: 241  Train Loss: 105.55521392822266 \n",
      "Epoch: 242  Train Loss: 111.1134033203125 \n",
      "Epoch: 243  Train Loss: 116.7602310180664 \n",
      "Epoch: 244  Train Loss: 116.5013427734375 \n",
      "Epoch: 245  Train Loss: 112.88471221923828 \n",
      "Epoch: 246  Train Loss: 113.54733276367188 \n",
      "Epoch: 247  Train Loss: 102.12091064453125 \n",
      "Epoch: 248  Train Loss: 123.0083236694336 \n",
      "Epoch: 249  Train Loss: 112.26419067382812 \n",
      "Epoch: 250  Train Loss: 113.78112030029297 \n",
      "Epoch: 251  Train Loss: 120.0263442993164 \n",
      "Epoch: 252  Train Loss: 105.06612396240234 \n",
      "Epoch: 253  Train Loss: 113.18167877197266 \n",
      "Epoch: 254  Train Loss: 117.9742202758789 \n",
      "Epoch: 255  Train Loss: 110.34396362304688 \n",
      "Epoch: 256  Train Loss: 114.64410400390625 \n",
      "Epoch: 257  Train Loss: 113.43976593017578 \n",
      "Epoch: 258  Train Loss: 115.2014389038086 \n",
      "Epoch: 259  Train Loss: 116.17586517333984 \n",
      "Epoch: 260  Train Loss: 112.7249984741211 \n",
      "Epoch: 261  Train Loss: 110.45954132080078 \n",
      "Epoch: 262  Train Loss: 110.61611938476562 \n",
      "Epoch: 263  Train Loss: 114.1377182006836 \n",
      "Epoch: 264  Train Loss: 113.12706756591797 \n",
      "Epoch: 265  Train Loss: 118.2984619140625 \n",
      "Epoch: 266  Train Loss: 112.9837646484375 \n",
      "Epoch: 267  Train Loss: 109.2353286743164 \n",
      "Epoch: 268  Train Loss: 114.1950454711914 \n",
      "Epoch: 269  Train Loss: 118.98860931396484 \n",
      "Epoch: 270  Train Loss: 116.92904663085938 \n",
      "Epoch: 271  Train Loss: 109.20156860351562 \n",
      "Epoch: 272  Train Loss: 114.4881820678711 \n",
      "Epoch: 273  Train Loss: 110.11145782470703 \n",
      "Epoch: 274  Train Loss: 114.51480102539062 \n",
      "Epoch: 275  Train Loss: 99.8911361694336 \n",
      "Epoch: 276  Train Loss: 116.4541015625 \n",
      "Epoch: 277  Train Loss: 116.05313873291016 \n",
      "Epoch: 278  Train Loss: 110.29788970947266 \n",
      "Epoch: 279  Train Loss: 114.01610565185547 \n",
      "Epoch: 280  Train Loss: 120.0054702758789 \n",
      "Epoch: 281  Train Loss: 107.17962646484375 \n",
      "Epoch: 282  Train Loss: 113.24323272705078 \n",
      "Epoch: 283  Train Loss: 112.97464752197266 \n",
      "Epoch: 284  Train Loss: 118.10210418701172 \n",
      "Epoch: 285  Train Loss: 115.40045166015625 \n",
      "Epoch: 286  Train Loss: 118.98492431640625 \n",
      "Epoch: 287  Train Loss: 111.72362518310547 \n",
      "Epoch: 288  Train Loss: 107.4089126586914 \n",
      "Epoch: 289  Train Loss: 102.32080841064453 \n",
      "Epoch: 290  Train Loss: 108.92355346679688 \n",
      "Epoch: 291  Train Loss: 114.94293975830078 \n",
      "Epoch: 292  Train Loss: 114.98812866210938 \n",
      "Epoch: 293  Train Loss: 121.4957046508789 \n",
      "Epoch: 294  Train Loss: 112.32648468017578 \n",
      "Epoch: 295  Train Loss: 115.92357635498047 \n",
      "Epoch: 296  Train Loss: 112.19812774658203 \n",
      "Epoch: 297  Train Loss: 112.19781494140625 \n",
      "Epoch: 298  Train Loss: 112.6509780883789 \n",
      "Epoch: 299  Train Loss: 115.20722198486328 \n",
      "Epoch: 300  Train Loss: 104.109375 \n",
      "Epoch: 301  Train Loss: 120.96334075927734 \n",
      "Epoch: 302  Train Loss: 111.88336181640625 \n",
      "Epoch: 303  Train Loss: 116.43359375 \n",
      "Epoch: 304  Train Loss: 106.37250518798828 \n",
      "Epoch: 305  Train Loss: 112.38994598388672 \n",
      "Epoch: 306  Train Loss: 108.43888092041016 \n",
      "Epoch: 307  Train Loss: 101.6994400024414 \n",
      "Epoch: 308  Train Loss: 105.49383544921875 \n",
      "Epoch: 309  Train Loss: 116.3365478515625 \n",
      "Epoch: 310  Train Loss: 107.96770477294922 \n",
      "Epoch: 311  Train Loss: 116.64083099365234 \n",
      "Epoch: 312  Train Loss: 104.53131103515625 \n",
      "Epoch: 313  Train Loss: 119.04715728759766 \n",
      "Epoch: 314  Train Loss: 105.47320556640625 \n",
      "Epoch: 315  Train Loss: 109.7034683227539 \n",
      "Epoch: 316  Train Loss: 112.21710968017578 \n",
      "Epoch: 317  Train Loss: 110.70034790039062 \n",
      "Epoch: 318  Train Loss: 108.87017059326172 \n",
      "Epoch: 319  Train Loss: 108.51516723632812 \n",
      "Epoch: 320  Train Loss: 114.5721664428711 \n",
      "Epoch: 321  Train Loss: 105.3802719116211 \n",
      "Epoch: 322  Train Loss: 120.48665618896484 \n",
      "Epoch: 323  Train Loss: 106.88506317138672 \n",
      "Epoch: 324  Train Loss: 118.54940032958984 \n",
      "Epoch: 325  Train Loss: 114.1286849975586 \n",
      "Epoch: 326  Train Loss: 113.91732788085938 \n",
      "Epoch: 327  Train Loss: 118.80340576171875 \n",
      "Epoch: 328  Train Loss: 107.7305908203125 \n",
      "Epoch: 329  Train Loss: 107.25131225585938 \n",
      "Epoch: 330  Train Loss: 107.95660400390625 \n",
      "Epoch: 331  Train Loss: 109.3792724609375 \n",
      "Epoch: 332  Train Loss: 107.8606948852539 \n",
      "Epoch: 333  Train Loss: 112.59502410888672 \n",
      "Epoch: 334  Train Loss: 111.71099090576172 \n",
      "Epoch: 335  Train Loss: 121.1895751953125 \n",
      "Epoch: 336  Train Loss: 115.58245849609375 \n",
      "Epoch: 337  Train Loss: 110.60537719726562 \n",
      "Epoch: 338  Train Loss: 111.00081634521484 \n",
      "Epoch: 339  Train Loss: 108.95702362060547 \n",
      "Epoch: 340  Train Loss: 102.75894165039062 \n",
      "Epoch: 341  Train Loss: 108.1156005859375 \n",
      "Epoch: 342  Train Loss: 108.57770538330078 \n",
      "Epoch: 343  Train Loss: 114.4720230102539 \n",
      "Epoch: 344  Train Loss: 101.82007598876953 \n",
      "Epoch: 345  Train Loss: 115.0015869140625 \n",
      "Epoch: 346  Train Loss: 121.764404296875 \n",
      "Epoch: 347  Train Loss: 106.61193084716797 \n",
      "Epoch: 348  Train Loss: 114.35113525390625 \n",
      "Epoch: 349  Train Loss: 118.10913848876953 \n",
      "Epoch: 350  Train Loss: 111.3337631225586 \n",
      "Epoch: 351  Train Loss: 111.7572250366211 \n",
      "Epoch: 352  Train Loss: 112.88259887695312 \n",
      "Epoch: 353  Train Loss: 116.2072982788086 \n",
      "Epoch: 354  Train Loss: 105.11786651611328 \n",
      "Epoch: 355  Train Loss: 112.99972534179688 \n",
      "Epoch: 356  Train Loss: 108.6859359741211 \n",
      "Epoch: 357  Train Loss: 119.10167694091797 \n",
      "Epoch: 358  Train Loss: 110.32555389404297 \n",
      "Epoch: 359  Train Loss: 111.36599731445312 \n",
      "Epoch: 360  Train Loss: 113.38256072998047 \n",
      "Epoch: 361  Train Loss: 108.4928970336914 \n",
      "Epoch: 362  Train Loss: 107.27572631835938 \n",
      "Epoch: 363  Train Loss: 107.90885162353516 \n",
      "Epoch: 364  Train Loss: 107.14240264892578 \n",
      "Epoch: 365  Train Loss: 102.4444351196289 \n",
      "Epoch: 366  Train Loss: 106.38623809814453 \n",
      "Epoch: 367  Train Loss: 107.01358032226562 \n",
      "Epoch: 368  Train Loss: 101.0747299194336 \n",
      "Epoch: 369  Train Loss: 109.852783203125 \n",
      "Epoch: 370  Train Loss: 112.53814697265625 \n",
      "Epoch: 371  Train Loss: 107.64270782470703 \n",
      "Epoch: 372  Train Loss: 109.36937713623047 \n",
      "Epoch: 373  Train Loss: 116.41583251953125 \n",
      "Epoch: 374  Train Loss: 112.27030181884766 \n",
      "Epoch: 375  Train Loss: 118.09296417236328 \n",
      "Epoch: 376  Train Loss: 104.35110473632812 \n",
      "Epoch: 377  Train Loss: 107.3669662475586 \n",
      "Epoch: 378  Train Loss: 104.70853424072266 \n",
      "Epoch: 379  Train Loss: 102.5549545288086 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 380  Train Loss: 110.5578842163086 \n",
      "Epoch: 381  Train Loss: 97.77740478515625 \n",
      "Epoch: 382  Train Loss: 106.2132568359375 \n",
      "Epoch: 383  Train Loss: 106.51978302001953 \n",
      "Epoch: 384  Train Loss: 105.19189453125 \n",
      "Epoch: 385  Train Loss: 98.39202117919922 \n",
      "Epoch: 386  Train Loss: 108.96674346923828 \n",
      "Epoch: 387  Train Loss: 108.72835540771484 \n",
      "Epoch: 388  Train Loss: 112.6445541381836 \n",
      "Epoch: 389  Train Loss: 106.93512725830078 \n",
      "Epoch: 390  Train Loss: 101.38158416748047 \n",
      "Epoch: 391  Train Loss: 100.07974243164062 \n",
      "Epoch: 392  Train Loss: 106.72088623046875 \n",
      "Epoch: 393  Train Loss: 102.71688079833984 \n",
      "Epoch: 394  Train Loss: 105.6919937133789 \n",
      "Epoch: 395  Train Loss: 99.8871078491211 \n",
      "Epoch: 396  Train Loss: 110.47110748291016 \n",
      "Epoch: 397  Train Loss: 101.80786895751953 \n",
      "Epoch: 398  Train Loss: 105.9005126953125 \n",
      "Epoch: 399  Train Loss: 104.6712875366211 \n",
      "Epoch: 400  Train Loss: 108.99971771240234 \n",
      "Epoch: 401  Train Loss: 107.04491424560547 \n",
      "Epoch: 402  Train Loss: 111.81111907958984 \n",
      "Epoch: 403  Train Loss: 111.37398529052734 \n",
      "Epoch: 404  Train Loss: 109.99758911132812 \n",
      "Epoch: 405  Train Loss: 105.61133575439453 \n",
      "Epoch: 406  Train Loss: 107.41988372802734 \n",
      "Epoch: 407  Train Loss: 103.27850341796875 \n",
      "Epoch: 408  Train Loss: 108.17900848388672 \n",
      "Epoch: 409  Train Loss: 106.15240478515625 \n",
      "Epoch: 410  Train Loss: 101.27117919921875 \n",
      "Epoch: 411  Train Loss: 100.15237426757812 \n",
      "Epoch: 412  Train Loss: 98.81170654296875 \n",
      "Epoch: 413  Train Loss: 108.34130096435547 \n",
      "Epoch: 414  Train Loss: 99.0665512084961 \n",
      "Epoch: 415  Train Loss: 96.41658782958984 \n",
      "Epoch: 416  Train Loss: 103.58635711669922 \n",
      "Epoch: 417  Train Loss: 102.71427154541016 \n",
      "Epoch: 418  Train Loss: 98.45488739013672 \n",
      "Epoch: 419  Train Loss: 98.92171478271484 \n",
      "Epoch: 420  Train Loss: 105.50778198242188 \n",
      "Epoch: 421  Train Loss: 104.2914810180664 \n",
      "Epoch: 422  Train Loss: 106.2974853515625 \n",
      "Epoch: 423  Train Loss: 109.31570434570312 \n",
      "Epoch: 424  Train Loss: 105.34378814697266 \n",
      "Epoch: 425  Train Loss: 99.90509796142578 \n",
      "Epoch: 426  Train Loss: 109.1090087890625 \n",
      "Epoch: 427  Train Loss: 105.82513427734375 \n",
      "Epoch: 428  Train Loss: 97.30156707763672 \n",
      "Epoch: 429  Train Loss: 105.10974884033203 \n",
      "Epoch: 430  Train Loss: 100.2711410522461 \n",
      "Epoch: 431  Train Loss: 99.66053009033203 \n",
      "Epoch: 432  Train Loss: 97.993896484375 \n",
      "Epoch: 433  Train Loss: 107.63848876953125 \n",
      "Epoch: 434  Train Loss: 105.93499755859375 \n",
      "Epoch: 435  Train Loss: 100.87334442138672 \n",
      "Epoch: 436  Train Loss: 107.0527572631836 \n",
      "Epoch: 437  Train Loss: 106.08124542236328 \n",
      "Epoch: 438  Train Loss: 102.63421630859375 \n",
      "Epoch: 439  Train Loss: 103.29379272460938 \n",
      "Epoch: 440  Train Loss: 98.7673568725586 \n",
      "Epoch: 441  Train Loss: 98.11280059814453 \n",
      "Epoch: 442  Train Loss: 98.36871337890625 \n",
      "Epoch: 443  Train Loss: 103.51248931884766 \n",
      "Epoch: 444  Train Loss: 99.58487701416016 \n",
      "Epoch: 445  Train Loss: 95.4288558959961 \n",
      "Epoch: 446  Train Loss: 94.1673355102539 \n",
      "Epoch: 447  Train Loss: 106.361328125 \n",
      "Epoch: 448  Train Loss: 105.72024536132812 \n",
      "Epoch: 449  Train Loss: 98.38904571533203 \n",
      "Epoch: 450  Train Loss: 106.2754898071289 \n",
      "Epoch: 451  Train Loss: 104.9610595703125 \n",
      "Epoch: 452  Train Loss: 97.78378295898438 \n",
      "Epoch: 453  Train Loss: 96.89302825927734 \n",
      "Epoch: 454  Train Loss: 99.54522705078125 \n",
      "Epoch: 455  Train Loss: 102.7358169555664 \n",
      "Epoch: 456  Train Loss: 96.58626556396484 \n",
      "Epoch: 457  Train Loss: 99.9898910522461 \n",
      "Epoch: 458  Train Loss: 102.43514251708984 \n",
      "Epoch: 459  Train Loss: 95.37213134765625 \n",
      "Epoch: 460  Train Loss: 97.82315826416016 \n",
      "Epoch: 461  Train Loss: 104.94635009765625 \n",
      "Epoch: 462  Train Loss: 105.30086517333984 \n",
      "Epoch: 463  Train Loss: 96.45925903320312 \n",
      "Epoch: 464  Train Loss: 99.21533203125 \n",
      "Epoch: 465  Train Loss: 98.13396453857422 \n",
      "Epoch: 466  Train Loss: 96.26214599609375 \n",
      "Epoch: 467  Train Loss: 96.02013397216797 \n",
      "Epoch: 468  Train Loss: 98.31689453125 \n",
      "Epoch: 469  Train Loss: 101.42937469482422 \n",
      "Epoch: 470  Train Loss: 98.1468276977539 \n",
      "Epoch: 471  Train Loss: 100.34793853759766 \n",
      "Epoch: 472  Train Loss: 93.57471466064453 \n",
      "Epoch: 473  Train Loss: 98.86199951171875 \n",
      "Epoch: 474  Train Loss: 101.0293197631836 \n",
      "Epoch: 475  Train Loss: 102.2778091430664 \n",
      "Epoch: 476  Train Loss: 98.06536865234375 \n",
      "Epoch: 477  Train Loss: 93.365478515625 \n",
      "Epoch: 478  Train Loss: 97.30028533935547 \n",
      "Epoch: 479  Train Loss: 96.174072265625 \n",
      "Epoch: 480  Train Loss: 98.86328887939453 \n",
      "Epoch: 481  Train Loss: 91.72661590576172 \n",
      "Epoch: 482  Train Loss: 95.43706512451172 \n",
      "Epoch: 483  Train Loss: 100.2823715209961 \n",
      "Epoch: 484  Train Loss: 91.0563735961914 \n",
      "Epoch: 485  Train Loss: 93.8449935913086 \n",
      "Epoch: 486  Train Loss: 94.89794158935547 \n",
      "Epoch: 487  Train Loss: 93.78485107421875 \n",
      "Epoch: 488  Train Loss: 94.93157958984375 \n",
      "Epoch: 489  Train Loss: 91.87677001953125 \n",
      "Epoch: 490  Train Loss: 92.2285385131836 \n",
      "Epoch: 491  Train Loss: 91.35195922851562 \n",
      "Epoch: 492  Train Loss: 97.0272216796875 \n",
      "Epoch: 493  Train Loss: 97.27275848388672 \n",
      "Epoch: 494  Train Loss: 95.44729614257812 \n",
      "Epoch: 495  Train Loss: 90.35318756103516 \n",
      "Epoch: 496  Train Loss: 100.15076446533203 \n",
      "Epoch: 497  Train Loss: 94.16280364990234 \n",
      "Epoch: 498  Train Loss: 95.53377532958984 \n",
      "Epoch: 499  Train Loss: 94.33719635009766 \n",
      "Epoch: 500  Train Loss: 97.61776733398438 \n",
      "Epoch: 501  Train Loss: 95.73190307617188 \n",
      "Epoch: 502  Train Loss: 94.3708724975586 \n",
      "Epoch: 503  Train Loss: 91.09925079345703 \n",
      "Epoch: 504  Train Loss: 94.31531524658203 \n",
      "Epoch: 505  Train Loss: 96.82231903076172 \n",
      "Epoch: 506  Train Loss: 95.23754119873047 \n",
      "Epoch: 507  Train Loss: 97.9363784790039 \n",
      "Epoch: 508  Train Loss: 97.05643463134766 \n",
      "Epoch: 509  Train Loss: 91.1972885131836 \n",
      "Epoch: 510  Train Loss: 94.8659439086914 \n",
      "Epoch: 511  Train Loss: 92.76776885986328 \n",
      "Epoch: 512  Train Loss: 103.21382904052734 \n",
      "Epoch: 513  Train Loss: 96.64859771728516 \n",
      "Epoch: 514  Train Loss: 100.35884857177734 \n",
      "Epoch: 515  Train Loss: 97.0694351196289 \n",
      "Epoch: 516  Train Loss: 93.37381744384766 \n",
      "Epoch: 517  Train Loss: 93.20348358154297 \n",
      "Epoch: 518  Train Loss: 97.77810668945312 \n",
      "Epoch: 519  Train Loss: 96.69168090820312 \n",
      "Epoch: 520  Train Loss: 93.8165054321289 \n",
      "Epoch: 521  Train Loss: 91.36934661865234 \n",
      "Epoch: 522  Train Loss: 94.3408432006836 \n",
      "Epoch: 523  Train Loss: 92.84844207763672 \n",
      "Epoch: 524  Train Loss: 87.65723419189453 \n",
      "Epoch: 525  Train Loss: 92.54940032958984 \n",
      "Epoch: 526  Train Loss: 93.56291961669922 \n",
      "Epoch: 527  Train Loss: 97.18791961669922 \n",
      "Epoch: 528  Train Loss: 86.39165496826172 \n",
      "Epoch: 529  Train Loss: 95.45209503173828 \n",
      "Epoch: 530  Train Loss: 96.4078140258789 \n",
      "Epoch: 531  Train Loss: 83.09844207763672 \n",
      "Epoch: 532  Train Loss: 93.68134307861328 \n",
      "Epoch: 533  Train Loss: 91.7503433227539 \n",
      "Epoch: 534  Train Loss: 90.56734466552734 \n",
      "Epoch: 535  Train Loss: 82.54855346679688 \n",
      "Epoch: 536  Train Loss: 83.88555145263672 \n",
      "Epoch: 537  Train Loss: 87.02294921875 \n",
      "Epoch: 538  Train Loss: 85.60091400146484 \n",
      "Epoch: 539  Train Loss: 90.3263168334961 \n",
      "Epoch: 540  Train Loss: 87.15502166748047 \n",
      "Epoch: 541  Train Loss: 95.65166473388672 \n",
      "Epoch: 542  Train Loss: 89.66851043701172 \n",
      "Epoch: 543  Train Loss: 91.5937271118164 \n",
      "Epoch: 544  Train Loss: 93.5080337524414 \n",
      "Epoch: 545  Train Loss: 96.2527084350586 \n",
      "Epoch: 546  Train Loss: 92.3965835571289 \n",
      "Epoch: 547  Train Loss: 93.42285919189453 \n",
      "Epoch: 548  Train Loss: 83.25405883789062 \n",
      "Epoch: 549  Train Loss: 82.92920684814453 \n",
      "Epoch: 550  Train Loss: 87.1063003540039 \n",
      "Epoch: 551  Train Loss: 86.94769287109375 \n",
      "Epoch: 552  Train Loss: 89.34627532958984 \n",
      "Epoch: 553  Train Loss: 86.32830810546875 \n",
      "Epoch: 554  Train Loss: 89.81929779052734 \n",
      "Epoch: 555  Train Loss: 87.29037475585938 \n",
      "Epoch: 556  Train Loss: 85.44503021240234 \n"
     ]
    }
   ],
   "source": [
    "kfold.cross_validation_evaluate(train_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 409,600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-harassment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
